{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9911893,"sourceType":"datasetVersion","datasetId":6090336}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:10.676263Z","iopub.execute_input":"2024-11-20T06:04:10.677157Z","iopub.status.idle":"2024-11-20T06:04:11.030992Z","shell.execute_reply.started":"2024-11-20T06:04:10.677114Z","shell.execute_reply":"2024-11-20T06:04:11.030158Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/customer-capstone/customer_capstone_project.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3. Sentiment Classification","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:13.480367Z","iopub.execute_input":"2024-11-20T06:04:13.481031Z","iopub.status.idle":"2024-11-20T06:04:13.485357Z","shell.execute_reply.started":"2024-11-20T06:04:13.480994Z","shell.execute_reply":"2024-11-20T06:04:13.484533Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## Read our final cleaned dataset \ndf = pd.read_csv(\"/kaggle/input/customer-capstone/customer_capstone_project.csv\")\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:14.263984Z","iopub.execute_input":"2024-11-20T06:04:14.264581Z","iopub.status.idle":"2024-11-20T06:04:14.914919Z","shell.execute_reply.started":"2024-11-20T06:04:14.264544Z","shell.execute_reply":"2024-11-20T06:04:14.913929Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       name          city  Business_stars  review_count  is_open  \\\n0  Spice 28  Philadelphia             4.0         822.0      1.0   \n1  Spice 28  Philadelphia             4.0         822.0      1.0   \n2  Spice 28  Philadelphia             4.0         822.0      1.0   \n\n                                          categories  \\\n0  ['Asian Fusion', 'Restaurants', 'American (New...   \n1  ['Asian Fusion', 'Restaurants', 'American (New...   \n2  ['Asian Fusion', 'Restaurants', 'American (New...   \n\n                                          attributes  review_stars  \\\n0  {'RestaurantsGoodForGroups': 'True', 'WiFi': \"...           4.0   \n1  {'RestaurantsGoodForGroups': 'True', 'WiFi': \"...           4.0   \n2  {'RestaurantsGoodForGroups': 'True', 'WiFi': \"...           4.0   \n\n   review_useful  review_funny  review_cool  \\\n0            1.0           0.0          0.0   \n1            1.0           0.0          0.0   \n2            1.0           0.0          0.0   \n\n                                         review_text  \\\n0  good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...   \n1  good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...   \n2  good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...   \n\n                                 review_cleaned_text  \\\n0  good ambience\\n\\ngood service\\n\\nordered \\n\\nf...   \n1  good ambience\\n\\ngood service\\n\\nordered \\n\\nf...   \n2  good ambience\\n\\ngood service\\n\\nordered \\n\\nf...   \n\n                            review_preprocessed_text  tip_compliment_count  \\\n0  goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...                   0.0   \n1  goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...                   0.0   \n2  goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...                   0.0   \n\n                                            tip_text  \\\n0  Started with a few appetizers - all good, thou...   \n1                                         Awesome!!!   \n2  Good food. There were things I liked and disli...   \n\n                                    tip_cleaned_text  \\\n0  started with a few appetizers  all good though...   \n1                                            awesome   \n2   good food there were things i liked and disliked   \n\n                               tip_preprocessed_text    year  month  \\\n0  startappetizer goodindianpancakestandmaincours...  2012.0   11.0   \n1                                            awesome  2013.0   12.0   \n2                           goodfoodthinglikedislike  2013.0    3.0   \n\n   review_sentiment        date  season  \n0          0.415278  2012-11-01   Rainy  \n1          0.415278  2013-12-01  Winter  \n2          0.415278  2013-03-01  Spring  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>city</th>\n      <th>Business_stars</th>\n      <th>review_count</th>\n      <th>is_open</th>\n      <th>categories</th>\n      <th>attributes</th>\n      <th>review_stars</th>\n      <th>review_useful</th>\n      <th>review_funny</th>\n      <th>review_cool</th>\n      <th>review_text</th>\n      <th>review_cleaned_text</th>\n      <th>review_preprocessed_text</th>\n      <th>tip_compliment_count</th>\n      <th>tip_text</th>\n      <th>tip_cleaned_text</th>\n      <th>tip_preprocessed_text</th>\n      <th>year</th>\n      <th>month</th>\n      <th>review_sentiment</th>\n      <th>date</th>\n      <th>season</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Spice 28</td>\n      <td>Philadelphia</td>\n      <td>4.0</td>\n      <td>822.0</td>\n      <td>1.0</td>\n      <td>['Asian Fusion', 'Restaurants', 'American (New...</td>\n      <td>{'RestaurantsGoodForGroups': 'True', 'WiFi': \"...</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...</td>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...</td>\n      <td>0.0</td>\n      <td>Started with a few appetizers - all good, thou...</td>\n      <td>started with a few appetizers  all good though...</td>\n      <td>startappetizer goodindianpancakestandmaincours...</td>\n      <td>2012.0</td>\n      <td>11.0</td>\n      <td>0.415278</td>\n      <td>2012-11-01</td>\n      <td>Rainy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spice 28</td>\n      <td>Philadelphia</td>\n      <td>4.0</td>\n      <td>822.0</td>\n      <td>1.0</td>\n      <td>['Asian Fusion', 'Restaurants', 'American (New...</td>\n      <td>{'RestaurantsGoodForGroups': 'True', 'WiFi': \"...</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...</td>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...</td>\n      <td>0.0</td>\n      <td>Awesome!!!</td>\n      <td>awesome</td>\n      <td>awesome</td>\n      <td>2013.0</td>\n      <td>12.0</td>\n      <td>0.415278</td>\n      <td>2013-12-01</td>\n      <td>Winter</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Spice 28</td>\n      <td>Philadelphia</td>\n      <td>4.0</td>\n      <td>822.0</td>\n      <td>1.0</td>\n      <td>['Asian Fusion', 'Restaurants', 'American (New...</td>\n      <td>{'RestaurantsGoodForGroups': 'True', 'WiFi': \"...</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>good ambience\\n\\ngood service\\n\\nOrdered: \\n\\n...</td>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>goodambience\\n\\ngoodservice\\n\\norder\\n\\nfishpe...</td>\n      <td>0.0</td>\n      <td>Good food. There were things I liked and disli...</td>\n      <td>good food there were things i liked and disliked</td>\n      <td>goodfoodthinglikedislike</td>\n      <td>2013.0</td>\n      <td>3.0</td>\n      <td>0.415278</td>\n      <td>2013-03-01</td>\n      <td>Spring</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# df.review_cleaned_text[3]\n# df.review_sentiment[3]\ndf.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:28.253705Z","iopub.execute_input":"2024-11-20T06:04:28.254085Z","iopub.status.idle":"2024-11-20T06:04:28.316441Z","shell.execute_reply.started":"2024-11-20T06:04:28.254054Z","shell.execute_reply":"2024-11-20T06:04:28.315547Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"## Convert Sentiment Scores to Categories\n\n# Function to convert sentiment score to categories\ndef convert_sentiment(score):\n    if score <= -0.1:\n        return 'negative'\n    elif -0.1 < score <= 0.1:\n        return 'neutral'\n    else:\n        return 'positive'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:29.700351Z","iopub.execute_input":"2024-11-20T06:04:29.700705Z","iopub.status.idle":"2024-11-20T06:04:29.705602Z","shell.execute_reply.started":"2024-11-20T06:04:29.700668Z","shell.execute_reply":"2024-11-20T06:04:29.704761Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Apply the function to the sentiment_score column\ndf['sentiment_label'] = df['review_sentiment'].apply(convert_sentiment)\n\n# Check the result\ndf_sentiment = df[['review_cleaned_text', 'sentiment_label']]\ndf_sentiment.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:04:53.437863Z","iopub.execute_input":"2024-11-20T06:04:53.438698Z","iopub.status.idle":"2024-11-20T06:04:53.456970Z","shell.execute_reply.started":"2024-11-20T06:04:53.438663Z","shell.execute_reply":"2024-11-20T06:04:53.456097Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                     review_cleaned_text sentiment_label\n10335  i ate here a few times before and was very ple...        positive\n10336  best food ever  have been trying hard to find ...        positive\n10337  best food ever  have been trying hard to find ...        positive\n10338  awesome dive bar  citywides here looooove love...        positive\n10339  i had the crispy chicken wrap with dijon and f...        positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_cleaned_text</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10335</th>\n      <td>i ate here a few times before and was very ple...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10336</th>\n      <td>best food ever  have been trying hard to find ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10337</th>\n      <td>best food ever  have been trying hard to find ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10338</th>\n      <td>awesome dive bar  citywides here looooove love...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10339</th>\n      <td>i had the crispy chicken wrap with dijon and f...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_sentiment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:05:00.364608Z","iopub.execute_input":"2024-11-20T06:05:00.365176Z","iopub.status.idle":"2024-11-20T06:05:00.381348Z","shell.execute_reply.started":"2024-11-20T06:05:00.365117Z","shell.execute_reply":"2024-11-20T06:05:00.379610Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                     review_cleaned_text sentiment_label\n0      good ambience\\n\\ngood service\\n\\nordered \\n\\nf...        positive\n1      good ambience\\n\\ngood service\\n\\nordered \\n\\nf...        positive\n2      good ambience\\n\\ngood service\\n\\nordered \\n\\nf...        positive\n3      good ambience\\n\\ngood service\\n\\nordered \\n\\nf...        positive\n4      good ambience\\n\\ngood service\\n\\nordered \\n\\nf...        positive\n...                                                  ...             ...\n10335  i ate here a few times before and was very ple...        positive\n10336  best food ever  have been trying hard to find ...        positive\n10337  best food ever  have been trying hard to find ...        positive\n10338  awesome dive bar  citywides here looooove love...        positive\n10339  i had the crispy chicken wrap with dijon and f...        positive\n\n[10340 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_cleaned_text</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good ambience\\n\\ngood service\\n\\nordered \\n\\nf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10335</th>\n      <td>i ate here a few times before and was very ple...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10336</th>\n      <td>best food ever  have been trying hard to find ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10337</th>\n      <td>best food ever  have been trying hard to find ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10338</th>\n      <td>awesome dive bar  citywides here looooove love...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10339</th>\n      <td>i had the crispy chicken wrap with dijon and f...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>10340 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"**Model Selection**\n\nFor sentiment classification, we will be using BERT from the Hugging Face transformers library.","metadata":{}},{"cell_type":"code","source":"# Import all the neccessary libraries\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:07:09.767461Z","iopub.execute_input":"2024-11-20T06:07:09.767909Z","iopub.status.idle":"2024-11-20T06:07:14.494586Z","shell.execute_reply.started":"2024-11-20T06:07:09.767876Z","shell.execute_reply":"2024-11-20T06:07:14.493549Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:07:40.980504Z","iopub.execute_input":"2024-11-20T06:07:40.981416Z","iopub.status.idle":"2024-11-20T06:07:42.276297Z","shell.execute_reply.started":"2024-11-20T06:07:40.981380Z","shell.execute_reply":"2024-11-20T06:07:42.275329Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05c92d6838e4cffb5879a6a5ff77abc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f681d7109d4513bc82fe8d21e8a07f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4942bc48857446c8b72af869dccfd96f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6d100239ab4b36ba6b10ff4065f024"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_df, test_df = train_test_split(df_sentiment, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:07:59.757197Z","iopub.execute_input":"2024-11-20T06:07:59.757552Z","iopub.status.idle":"2024-11-20T06:07:59.766179Z","shell.execute_reply.started":"2024-11-20T06:07:59.757520Z","shell.execute_reply":"2024-11-20T06:07:59.765295Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Tokenize the text\ndef tokenize_function(text):\n    return tokenizer(text, padding='max_length', truncation=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:08:12.606371Z","iopub.execute_input":"2024-11-20T06:08:12.606744Z","iopub.status.idle":"2024-11-20T06:08:12.611437Z","shell.execute_reply.started":"2024-11-20T06:08:12.606717Z","shell.execute_reply":"2024-11-20T06:08:12.610404Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Apply tokenization\ntrain_encodings = tokenizer(list(train_df['review_cleaned_text']), padding=True, truncation=True, max_length=128)\ntest_encodings = tokenizer(list(test_df['review_cleaned_text']), padding=True, truncation=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:08:29.191980Z","iopub.execute_input":"2024-11-20T06:08:29.192672Z","iopub.status.idle":"2024-11-20T06:08:48.781316Z","shell.execute_reply.started":"2024-11-20T06:08:29.192611Z","shell.execute_reply":"2024-11-20T06:08:48.780421Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Convert Data into PyTorch Datasets**\n\nYou need to convert the tokenized data into PyTorch datasets for training the model.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass SentimentDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Convert data to PyTorch datasets\ntrain_labels = [1 if label == 'positive' else 0 for label in train_df['sentiment_label']]  # Binary labels\ntest_labels = [1 if label == 'positive' else 0 for label in test_df['sentiment_label']]\n\ntrain_dataset = SentimentDataset(train_encodings, train_labels)\ntest_dataset = SentimentDataset(test_encodings, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:12:13.866825Z","iopub.execute_input":"2024-11-20T06:12:13.867591Z","iopub.status.idle":"2024-11-20T06:12:13.875686Z","shell.execute_reply.started":"2024-11-20T06:12:13.867536Z","shell.execute_reply":"2024-11-20T06:12:13.874706Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# BERT Model","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',           # Output directory\n    num_train_epochs=3,               # Number of training epochs\n    per_device_train_batch_size=8,    # Batch size per device during training\n    per_device_eval_batch_size=16,    # Batch size for evaluation\n    warmup_steps=500,                 # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                # Strength of weight decay\n    logging_dir='./logs',             # Directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",      # Evaluation strategy to use\n    save_strategy=\"epoch\",            # Save strategy must match the evaluation strategy\n    load_best_model_at_end=True,      # Load the best model when finished training\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:14:50.410620Z","iopub.execute_input":"2024-11-20T06:14:50.411020Z","iopub.status.idle":"2024-11-20T06:24:06.308393Z","shell.execute_reply.started":"2024-11-20T06:14:50.410987Z","shell.execute_reply":"2024-11-20T06:24:06.307576Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111351908888941, max=1.0)â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6daaa74f6509495687232e407a803442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241120_061511-5zne1ohl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/luciferstarc2104-exl-/huggingface/runs/5zne1ohl' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/luciferstarc2104-exl-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/luciferstarc2104-exl-/huggingface' target=\"_blank\">https://wandb.ai/luciferstarc2104-exl-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/luciferstarc2104-exl-/huggingface/runs/5zne1ohl' target=\"_blank\">https://wandb.ai/luciferstarc2104-exl-/huggingface/runs/5zne1ohl</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1551' max='1551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1551/1551 08:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.114700</td>\n      <td>0.120844</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.025500</td>\n      <td>0.079994</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000300</td>\n      <td>0.103557</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1551, training_loss=0.12770566255221463, metrics={'train_runtime': 553.5093, 'train_samples_per_second': 44.834, 'train_steps_per_second': 2.802, 'total_flos': 1632340987453440.0, 'train_loss': 0.12770566255221463, 'epoch': 3.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Evaluate Model (BERT)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Evaluate the model\npredictions, labels, _ = trainer.predict(test_dataset)\n\n# Convert logits to predicted labels\npredicted_labels = predictions.argmax(axis=-1)\n\n# Calculate metrics\naccuracy = accuracy_score(labels, predicted_labels)\nprecision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average='binary')\n\n# Print evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:25:16.496156Z","iopub.execute_input":"2024-11-20T06:25:16.496530Z","iopub.status.idle":"2024-11-20T06:25:27.600818Z","shell.execute_reply.started":"2024-11-20T06:25:16.496497Z","shell.execute_reply":"2024-11-20T06:25:27.599843Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.9807\nPrecision: 0.9853\nRecall: 0.9911\nF1-Score: 0.9882\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:28:23.154512Z","iopub.execute_input":"2024-11-20T06:28:23.154905Z","iopub.status.idle":"2024-11-20T06:28:23.186737Z","shell.execute_reply.started":"2024-11-20T06:28:23.154875Z","shell.execute_reply":"2024-11-20T06:28:23.185803Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialize RoBERTa tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Tokenize the text for RoBERTa input\ntrain_encodings = tokenizer(list(train_df['review_cleaned_text']), padding=True, truncation=True, max_length=128)\ntest_encodings = tokenizer(list(test_df['review_cleaned_text']), padding=True, truncation=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:29:05.903734Z","iopub.execute_input":"2024-11-20T06:29:05.904085Z","iopub.status.idle":"2024-11-20T06:29:13.217102Z","shell.execute_reply.started":"2024-11-20T06:29:05.904056Z","shell.execute_reply":"2024-11-20T06:29:13.215948Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b334d130314d23ad7da36b0042f390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf961b877f944906821b220de7fc2f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7ee48af9f24c3f84046979815ecaf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04eb8d725972443d8ac26fc66ce44564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c6d3b5a25a42e299c8b5f62f4fb635"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Convert labels to binary (positive=1, negative=0)\ntrain_labels = [1 if label == 'positive' else 0 for label in train_df['sentiment_label']]  # Binary labels\ntest_labels = [1 if label == 'positive' else 0 for label in test_df['sentiment_label']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:30:22.215964Z","iopub.execute_input":"2024-11-20T06:30:22.216314Z","iopub.status.idle":"2024-11-20T06:30:22.224056Z","shell.execute_reply.started":"2024-11-20T06:30:22.216282Z","shell.execute_reply":"2024-11-20T06:30:22.223182Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Convert data to PyTorch datasets\ntrain_dataset = SentimentDataset(train_encodings, train_labels)\ntest_dataset = SentimentDataset(test_encodings, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:30:52.200499Z","iopub.execute_input":"2024-11-20T06:30:52.201354Z","iopub.status.idle":"2024-11-20T06:30:52.206303Z","shell.execute_reply.started":"2024-11-20T06:30:52.201317Z","shell.execute_reply":"2024-11-20T06:30:52.205327Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Load pre-trained RoBERTa model\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',           # Output directory\n    num_train_epochs=3,               # Number of training epochs\n    per_device_train_batch_size=8,    # Batch size per device during training\n    per_device_eval_batch_size=16,    # Batch size for evaluation\n    warmup_steps=500,                 # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                # Strength of weight decay\n    logging_dir='./logs',             # Directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",      # Evaluate every epoch\n    save_strategy=\"epoch\",            # Save model every epoch\n    load_best_model_at_end=True,      # Load the best model when finished training\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:31:27.071588Z","iopub.execute_input":"2024-11-20T06:31:27.071995Z","iopub.status.idle":"2024-11-20T06:40:55.967072Z","shell.execute_reply.started":"2024-11-20T06:31:27.071962Z","shell.execute_reply":"2024-11-20T06:40:55.966218Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1551' max='1551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1551/1551 09:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.249300</td>\n      <td>0.242949</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.104900</td>\n      <td>0.131745</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.043400</td>\n      <td>0.091849</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1551, training_loss=0.1910823626892742, metrics={'train_runtime': 567.9482, 'train_samples_per_second': 43.694, 'train_steps_per_second': 2.731, 'total_flos': 1632340987453440.0, 'train_loss': 0.1910823626892742, 'epoch': 3.0})"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# Evaluate Model (RoBERTa)","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\npredictions, labels, _ = trainer.predict(test_dataset)\n\n# Convert logits to predicted labels\npredicted_labels = predictions.argmax(axis=-1)\n\n# Calculate metrics\naccuracy = accuracy_score(labels, predicted_labels)\nprecision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average='binary')\n\n# Print evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:40:55.968450Z","iopub.execute_input":"2024-11-20T06:40:55.968788Z","iopub.status.idle":"2024-11-20T06:41:07.255584Z","shell.execute_reply.started":"2024-11-20T06:40:55.968760Z","shell.execute_reply":"2024-11-20T06:41:07.254646Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.9821\nPrecision: 0.9842\nRecall: 0.9941\nF1-Score: 0.9891\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# T5 Model","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:45:25.742155Z","iopub.execute_input":"2024-11-20T06:45:25.742530Z","iopub.status.idle":"2024-11-20T06:45:25.748294Z","shell.execute_reply.started":"2024-11-20T06:45:25.742495Z","shell.execute_reply":"2024-11-20T06:45:25.747192Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Define a function to preprocess the data\ndef preprocess_data(df):\n    return [\n        {\n            'input_text': f\"Classify sentiment: {text}\",\n            'target_text': 'positive' if sentiment == 'positive' else 'negative'\n        }\n        for text, sentiment in zip(df['review_cleaned_text'], df['sentiment_label'])\n    ]\n\n# Preprocess the data\ntrain_data = preprocess_data(train_df)\ntest_data = preprocess_data(test_df)\n\n# Convert to datasets\ntrain_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\ntest_dataset = Dataset.from_pandas(pd.DataFrame(test_data))\n\n# Initialize the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\n\n# Tokenize the data\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples['input_text'], padding=True, truncation=True, max_length=128)\n    labels = tokenizer(examples['target_text'], padding=True, truncation=True, max_length=32)\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Load the pre-trained T5 model\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small',num_labels=2)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T07:27:40.296116Z","iopub.execute_input":"2024-11-20T07:27:40.296985Z","iopub.status.idle":"2024-11-20T07:32:14.457041Z","shell.execute_reply.started":"2024-11-20T07:27:40.296951Z","shell.execute_reply":"2024-11-20T07:32:14.456071Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8272 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce51f4d82ec544409ad5bc8ecff27b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2068 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee10cd7f3c864adfbbdccab877262cab"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1551' max='1551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1551/1551 04:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.346800</td>\n      <td>0.116349</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.094900</td>\n      <td>0.087562</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.079800</td>\n      <td>0.081088</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1551, training_loss=0.17046615756149833, metrics={'train_runtime': 266.3863, 'train_samples_per_second': 93.158, 'train_steps_per_second': 5.822, 'total_flos': 839660536332288.0, 'train_loss': 0.17046615756149833, 'epoch': 3.0})"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\noutput = trainer.predict(test_dataset)\n\n# Access the predictions (logits) from the output\nlogits = output.predictions\n\n# Check the structure of logits if it is a tuple\n# If logits are a tuple, extract the actual logits tensor\nif isinstance(logits, tuple):\n    logits = logits[0]\n\n# Apply argmax to the logits to get the predicted class labels\npredicted_labels = logits.argmax(axis=-1)  # Use 'axis=-1' to get the predicted class\n\n\n# Flatten the labels and predicted labels if they are multi-output/multilabel\npredicted_labels_flat = predicted_labels.flatten()\ntrue_labels_flat = output.label_ids.flatten()\n\n# Now calculate the metrics\naccuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat)\n\n# Print evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision.mean():.4f}\")\nprint(f\"Recall: {recall.mean():.4f}\")\nprint(f\"F1-Score: {f1.mean():.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T07:45:35.047413Z","iopub.execute_input":"2024-11-20T07:45:35.048393Z","iopub.status.idle":"2024-11-20T07:45:41.230881Z","shell.execute_reply.started":"2024-11-20T07:45:35.048355Z","shell.execute_reply":"2024-11-20T07:45:41.229853Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.9734\nPrecision: 0.9538\nRecall: 0.9228\nF1-Score: 0.9368\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# 4. Topic Modeling","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}